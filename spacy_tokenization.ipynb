{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ulfat\n",
      "visited\n",
      "Puran\n",
      "Dhaka\n",
      ".\n",
      "he\n",
      "liked\n",
      "the\n",
      "kachi\n",
      "from\n",
      "Bukhara\n",
      "Kacchi\n",
      "and\n",
      "Biriyani\n",
      "house\n",
      ".\n",
      "The\n",
      "biriyani\n",
      "was\n",
      "200\n",
      "taka\n",
      ".\n",
      "I\n",
      "also\n",
      "had\n",
      "one\n",
      "glass\n",
      "of\n",
      "beauty\n",
      "lacchi\n"
     ]
    }
   ],
   "source": [
    "# creating a blank english language component\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Ulfat visited Puran Dhaka. he liked the kachi from Bukhara Kacchi and Biriyani house. The biriyani was 200 taka. I also had one glass of beauty lacchi\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ulfat"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the tokens can also be presented using python list\n",
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"Let's go to N.Y!\")\n",
    "\n",
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N.Y"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing object in nlp\n",
    "span = doc2[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s go to N.Y\n"
     ]
    }
   ],
   "source": [
    "print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ulfat\n"
     ]
    }
   ],
   "source": [
    "token0 = doc[0]\n",
    "print(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check the methods under the token class and use some of the methods\n",
    "\n",
    "dir(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to check different text types\n",
    "token0.like_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ulfat -> index:  0 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "visited -> index:  1 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "Puran -> index:  2 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "Dhaka -> index:  3 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      ". -> index:  4 is_alpha: False is_punct: True is_currency: False like_num: False\n",
      "he -> index:  5 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "liked -> index:  6 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "the -> index:  7 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "kachi -> index:  8 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "from -> index:  9 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "Bukhara -> index:  10 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "Kacchi -> index:  11 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "and -> index:  12 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "Biriyani -> index:  13 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "house -> index:  14 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      ". -> index:  15 is_alpha: False is_punct: True is_currency: False like_num: False\n",
      "The -> index:  16 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "biriyani -> index:  17 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "was -> index:  18 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "200 -> index:  19 is_alpha: False is_punct: False is_currency: False like_num: True\n",
      "taka -> index:  20 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      ". -> index:  21 is_alpha: False is_punct: True is_currency: False like_num: False\n",
      "I -> index:  22 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "also -> index:  23 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "had -> index:  24 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "one -> index:  25 is_alpha: True is_punct: False is_currency: False like_num: True\n",
      "glass -> index:  26 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "of -> index:  27 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "beauty -> index:  28 is_alpha: True is_punct: False is_currency: False like_num: False\n",
      "lacchi -> index:  29 is_alpha: True is_punct: False is_currency: False like_num: False\n"
     ]
    }
   ],
   "source": [
    "# to check a number of functions let us run a for loop\n",
    "for token in doc:\n",
    "    print(token, \"->\", \"index: \", token.i,\n",
    "            \"is_alpha:\", token.is_alpha, \n",
    "            \"is_punct:\", token.is_punct, \n",
    "            \"is_currency:\", token.is_currency, \n",
    "            \"like_num:\", token.like_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['North South University, CSE499A Group #3 information \\n',\n",
       " '====================================================\\n',\n",
       " '\\n',\n",
       " 'Name      birth day        email\\n',\n",
       " '----      ---------        -----\\n',\n",
       " 'Ulfat     05/03/00        ulfat@nsu.edu\\n',\n",
       " 'Shanto    01/08/99        shanto@nsu.edu\\n',\n",
       " 'Moriom    22/02/54        moriom@ctg.com']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"students.txt.\") as f:\n",
    "    text = f.readlines()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'North South University, CSE499A Group #3 information \\n ====================================================\\n \\n Name      birth day        email\\n ----      ---------        -----\\n Ulfat     05/03/00        ulfat@nsu.edu\\n Shanto    01/08/99        shanto@nsu.edu\\n Moriom    22/02/54        moriom@ctg.com'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us convert the array of text into a single one\n",
    "\n",
    "text = ' '.join(text)\n",
    "text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ulfat@nsu.edu, shanto@nsu.edu, moriom@ctg.com]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us take out the emails from our student list\n",
    "doc3 = nlp(text)\n",
    "emails = []\n",
    "for token in doc3:\n",
    "    if token.like_email:\n",
    "        emails.append(token)\n",
    "\n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ভাই\n",
      "ভাই\n",
      "হোটেল\n",
      "এর\n",
      "সকালের\n",
      "পরোটা\n",
      "এবং\n",
      "চিকেন\n",
      "সুপ\n",
      "টা\n",
      "যথেষ্ট\n",
      "ভাল\n",
      "মানের\n",
      "ছিল\n",
      "|\n"
     ]
    }
   ],
   "source": [
    "# let us try in bangla\n",
    "\n",
    "nlp_eng = spacy.blank(\"bn\")\n",
    "\n",
    "doc = nlp(\"ভাই ভাই হোটেল এর সকালের পরোটা এবং চিকেন সুপ টা যথেষ্ট ভাল মানের ছিল |\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "তবে False False\n",
      "পরোটার False False\n",
      "দাম False False\n",
      "ছিল False False\n",
      "২০ False True\n",
      "টাকা False False\n",
      "( False False\n",
      "৳ True False\n",
      ") False False\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"তবে পরোটার দাম ছিল ২০ টাকা (৳)\")\n",
    "\n",
    "for token in doc2:\n",
    "    print(token, token.is_currency, token.like_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customizing tokenizer to split bangla\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"পরোটার\", [\n",
    "         {ORTH: \"পরোটা\"}, \n",
    "         {ORTH: \"র\"}\n",
    "])\n",
    "\n",
    "doc_beng = nlp(\"তবে পরোটার দাম ছিল ২০ টাকা (৳)\")\n",
    "\n",
    "tokens = [token.text for token in doc_beng]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# customizing tokenizer\n",
    "\n",
    "doc = nlp('gimme double cheese extra large healthy pizza')\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# customize the nlp object of spacy\n",
    "# basically spliting it, changin actual text is not allowed\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\", [\n",
    "         {ORTH: \"gim\"}, \n",
    "         {ORTH: \"me\"}\n",
    "])\n",
    "\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us check the nlp pipeline and try to build a pipeline with some components\n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ভাই ভাই হোটেল\n",
      "এর সকালের পরোটা এবং চিকেন সুপ টা\n",
      "যথেষ্ট ভাল মানের ছিল | তবে\n",
      "পরোটার দাম ছিল ২০ (৳)\n"
     ]
    }
   ],
   "source": [
    "# to add a component manually in a pipeline\n",
    "#nlp.add_pipe(\"pipe_name\")\n",
    "# bug  (separating depending on words like 'এর', 'টা')\n",
    "doc = nlp(\"ভাই ভাই হোটেল এর সকালের পরোটা এবং চিকেন সুপ টা যথেষ্ট ভাল মানের ছিল | তবে পরোটার দাম ছিল ২০ (৳)\")\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Md Ulfat Tahsin.\n",
      "I am currently study CSE at NSU.\n",
      "I am passionate about teaching\n"
     ]
    }
   ],
   "source": [
    "doc_eng = nlp(\"My name is Md Ulfat Tahsin. I am currently study CSE at NSU. I am passionate about teaching\")\n",
    "\n",
    "for sentence in doc_eng.sents:\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "723d4b7bc280cd31fdada53ad6420192b9a3a8d60631096143cc718cb9440dc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
